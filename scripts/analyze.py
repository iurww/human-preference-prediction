import json
import pandas as pd
from transformers import AutoTokenizer
from tqdm import tqdm

def parse_json_field(field):
    """è§£æ JSON æ ¼å¼çš„å­—æ®µ"""
    try:
        parsed = json.loads(field)
        if isinstance(parsed, list):
            return ' '.join(str(x) for x in parsed)
        return str(parsed)
    except:
        return str(field)

def get_token_length(row, tokenizer):
    """è®¡ç®—å•ä¸ªæ ·æœ¬çš„tokené•¿åº¦"""
    prompt = parse_json_field(row['prompt'])
    response_a = parse_json_field(row['response_a'])
    response_b = parse_json_field(row['response_b'])
    
    # æ„å»ºå®Œæ•´æ–‡æœ¬
    text = f"Prompt: {prompt}\n\nResponse A: {response_a}\n\nResponse B: {response_b}"
    
    # Tokenize
    tokens = tokenizer(text, add_special_tokens=True)
    prompt_tokens = tokenizer(prompt, add_special_tokens=False)
    response_a_tokens = tokenizer(response_a, add_special_tokens=False)
    response_b_tokens = tokenizer(response_b, add_special_tokens=False)
    
    return len(tokens['input_ids']), len(prompt_tokens['input_ids']), len(response_a_tokens['input_ids']), len(response_b_tokens['input_ids'])

def filter_long_samples(input_file, output_file, model_path, max_length=1024):
    """
    ç­›é€‰å‡ºtokenizeåè¶…è¿‡max_lengthçš„æ ·æœ¬
    
    Args:
        input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        model_path: tokenizeræ¨¡å‹è·¯å¾„
        max_length: é•¿åº¦é˜ˆå€¼
    """
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_file}")
    df = pd.read_csv(input_file)
    print(f"âœ… æ€»æ ·æœ¬æ•°: {len(df)}")
    
    print(f"\nğŸ”§ åŠ è½½tokenizer: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    print(f"\nâ³ å¼€å§‹ç­›é€‰è¶…è¿‡ {max_length} tokens çš„æ ·æœ¬...")
    
    long_samples = []
    token_lengths = []
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†ä¸­"):
        length, prompt_length, response_a_length, response_b_length = get_token_length(row, tokenizer)
        token_lengths.append(length)
        
        if length > max_length:
            # æ·»åŠ é•¿åº¦ä¿¡æ¯åˆ—
            row_dict = row.to_dict()
            row_dict['token_length'] = length
            row_dict['prompt_length'] = prompt_length
            row_dict['response_a_length'] = response_a_length
            row_dict['response_b_length'] = response_b_length
            long_samples.append(row_dict)
    
    # åˆ›å»ºæ–°çš„DataFrame
    long_df = pd.DataFrame(long_samples)
    
    # æŒ‰é•¿åº¦é™åºæ’åº(æœ€é•¿çš„åœ¨æœ€å‰é¢)
    long_df = long_df.sort_values('token_length', ascending=False)
    
    # ä¿å­˜åˆ°CSV
    long_df.to_csv(output_file, index=False, encoding='utf-8')
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"  è¶…è¿‡ {max_length} çš„æ ·æœ¬æ•°: {len(long_samples)}")
    print(f"  å æ¯”: {len(long_samples)/len(df)*100:.2f}%")
    print(f"  æœ€é•¿æ ·æœ¬: {max(token_lengths)} tokens")
    print(f"  æœ€çŸ­è¶…é•¿æ ·æœ¬: {long_df['token_length'].min()} tokens" if len(long_samples) > 0 else "")
    print(f"\nâœ… å·²ä¿å­˜åˆ°: {output_file}")
    print(f"   (æŒ‰tokené•¿åº¦é™åºæ’åˆ—,æœ€é•¿çš„åœ¨æœ€å‰é¢)")
    
    # é¢å¤–ä¿å­˜ä¸€ä¸ªç®€åŒ–ç‰ˆ(åªåŒ…å«å…³é”®ä¿¡æ¯,æ–¹ä¾¿æŸ¥çœ‹)
    if len(long_samples) > 0:
        simplified_output = output_file.replace('.csv', '_simplified.csv')
        simplified_df = long_df[['id', 'token_length', 'prompt', 'response_a', 'response_b', 
                                  'prompt_length', 'response_a_length', 'response_b_length',]]
        
        # æˆªæ–­æ–‡æœ¬é¢„è§ˆ(åªæ˜¾ç¤ºå‰100å­—ç¬¦)
        for col in ['prompt', 'response_a', 'response_b']:
            simplified_df[col + '_preview'] = simplified_df[col].apply(
                lambda x: str(x)[:100] + '...' if len(str(x)) > 100 else str(x)
            )
            simplified_df = simplified_df.drop(columns=[col])
        
        simplified_df.to_csv(simplified_output, index=False, encoding='utf-8')
        print(f"   (ç®€åŒ–ç‰ˆå·²ä¿å­˜åˆ°: {simplified_output})")

if __name__ == '__main__':
    # é…ç½®å‚æ•°
    INPUT_FILE = 'data/train_new.csv'
    OUTPUT_FILE = 'data/train_long_samples.csv'
    MODEL_PATH = './models/deberta'
    MAX_LENGTH = 1024
    
    filter_long_samples(
        input_file=INPUT_FILE,
        output_file=OUTPUT_FILE,
        model_path=MODEL_PATH,
        max_length=MAX_LENGTH
    )