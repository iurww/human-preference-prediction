# Human Preference Prediction

ä½¿ç”¨ DeBERTa-v3 æ¨¡å‹é¢„æµ‹äººç±»å¯¹ LLM å“åº”çš„åå¥½ã€‚

## é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ checkpoints
â”‚   â””â”€â”€ best_model
â”œâ”€â”€ configs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ configs.py
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â””â”€â”€ random_seed.py
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ test.csv
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ train_short.csv
â”œâ”€â”€ dataset
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ human_preference_dataset.py
â”‚   â””â”€â”€ human_preference_test_dataset.py
â”œâ”€â”€ models 
â”‚   â””â”€â”€ deberta
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ spm.model
â”‚       â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ analyze.py
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ download_data.py
â”‚   â”œâ”€â”€ find_dirty.py
â”‚   â”œâ”€â”€ infrrence.py
â”‚   â””â”€â”€ train_advanced.py
â”œâ”€â”€ README.md
â”œâ”€â”€ setup.sh
â”œâ”€â”€ inference.py
â””â”€â”€train.py
```

- checkpoint/ä¿å­˜è®­ç»ƒæ¨¡å‹
- configs/è®­ç»ƒé…ç½®
- dataset/æ•°æ®é›†å¤„ç†
- data/æ•°æ®é›†-kaggleä¸‹è½½
- models/æ¨¡å‹æ–‡ä»¶-huggingfaceä¸‹è½½
- train.pyè®­ç»ƒ
- inferenceæ¨ç†



## å¿«é€Ÿå¼€å§‹

```bash
conda create --name mlg3 python=3.13
conda activate mlg3
pip install torch transformers datasets pandas numpy scikit-learn wandb tqdm kaggle sentencepiece matplotlib

wandb login
```



## ä½¿ç”¨æ–¹æ³•

### 1. ä¸‹è½½æ•°æ®

æ”¾åœ¨dataç›®å½•ä¸‹,train.csv/test.csv

### 2. è®­ç»ƒæ¨¡å‹

```bash
python train.py
```

**åŠŸèƒ½ç‰¹æ€§ï¼š**

- âœ… ä½¿ç”¨ logging æ¨¡å—è®°å½•æ‰€æœ‰ä¿¡æ¯
- âœ… è‡ªåŠ¨åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼ˆ90%/10%ï¼‰
- âœ… WandB è®°å½•æ‰€æœ‰è®­ç»ƒæŒ‡æ ‡
- âœ… è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- âœ… ç”Ÿæˆ train.log æ—¥å¿—æ–‡ä»¶

**è®­ç»ƒæ—¥å¿—ç¤ºä¾‹ï¼š**
```
2024-01-01 10:00:00 - __main__ - INFO - ============================================================
2024-01-01 10:00:00 - __main__ - INFO - Starting training with configuration:
2024-01-01 10:00:00 - __main__ - INFO -   model_name: microsoft/deberta-v3-base
2024-01-01 10:00:00 - __main__ - INFO -   batch_size: 8
2024-01-01 10:00:00 - __main__ - INFO -   learning_rate: 2e-05
...
2024-01-01 10:00:05 - __main__ - INFO - Using device: cuda
2024-01-01 10:00:05 - __main__ - INFO - GPU: NVIDIA GeForce RTX 3090
2024-01-01 10:00:06 - __main__ - INFO - Train size: 41383, Validation size: 4598
...
2024-01-01 10:15:23 - __main__ - INFO - [Train] Loss: 0.8234, Log Loss: 0.8156
2024-01-01 10:16:45 - __main__ - INFO - [Val]   Loss: 0.7845, Log Loss: 0.7823
2024-01-01 10:16:45 - __main__ - INFO - ğŸ‰ New best validation log loss: 0.7823
```

### 3. ç”Ÿæˆé¢„æµ‹

```bash
python inference.py
```

**è¾“å‡ºæ—¥å¿—ç¤ºä¾‹ï¼š**
```
2024-01-01 11:00:00 - __main__ - INFO - ============================================================
2024-01-01 11:00:00 - __main__ - INFO - Starting inference
2024-01-01 11:00:00 - __main__ - INFO - ============================================================
2024-01-01 11:00:01 - __main__ - INFO - Using device: cuda
2024-01-01 11:00:02 - __main__ - INFO - Test samples: 11496
...
2024-01-01 11:02:34 - __main__ - INFO - Submission saved to submission.csv
2024-01-01 11:02:34 - __main__ - INFO - ============================================================
2024-01-01 11:02:34 - __main__ - INFO - Prediction Statistics:
2024-01-01 11:02:34 - __main__ - INFO -   Model A wins (avg): 0.3245
2024-01-01 11:02:34 - __main__ - INFO -   Model B wins (avg): 0.4123
2024-01-01 11:02:34 - __main__ - INFO -   Ties (avg): 0.2632
2024-01-01 11:02:34 - __main__ - INFO - ============================================================
```

### 4. æäº¤åˆ° Kaggle

```bash
kaggle competitions submit -c human-preference -f submission.csv -m "DeBERTa-v3-base submission"
```



## æ¨¡å‹é…ç½®

åœ¨ `configs/configs.py` ä¸­ä¿®æ”¹é…ç½®ï¼š

```python
DEFAULT_CONFIG = {
    'model_name': './models/deberta',
    'train_dataset_path': './data/train.csv',
    'test_dataset_path': './data/test.csv',
    'log_dir': './logs',
    'checkpoint_dir': './checkpoints',
    
    'max_length': 1024,
    'prompt_ratio': 0.3,
    
    'use_amp': False,
    'use_lora': False,
    'batch_size': 2,
    'learning_rate': 1e-5,
    'num_epochs': 40,
    'warmup_ratio': 0.1,
    'weight_decay': 0.01,
    'seed': 42,
    'val_rate': 0.01
}
```



## è¯„ä¼°æŒ‡æ ‡

ä½¿ç”¨ **Log Loss** è¿›è¡Œè¯„ä¼°ï¼š

$$
\text{LogLoss} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{c \in \{A,B,TIE\}} \mathbf{1}(y_i = c) \log p_{i,c}
$$

- $N$: æ ·æœ¬æ•°é‡
- $y_i$: çœŸå®æ ‡ç­¾
- $p_{i,c}$: é¢„æµ‹æ¦‚ç‡

**è¶Šå°è¶Šå¥½** âœ…


## å‚è€ƒèµ„æº

- [DeBERTa-v3 è®ºæ–‡](https://arxiv.org/abs/2111.09543)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [WandB æ–‡æ¡£](https://docs.wandb.ai/)
- [Kaggle ç«èµ›é¡µé¢](https://www.kaggle.com/c/human-preference)



## TODO

- amp
- æ¢æ¨¡å‹
- lora
- æ•°æ®é‡Œçš„unkæ€ä¹ˆå¤„ç†
- æ•°æ®é‡Œæ˜¯å¦æœ‰ç›¸åŒé—®é¢˜ä¸åŒæ¨¡å‹å›ç­”
- é•¿æ ·æœ¬æ€ä¹ˆå¤„ç†,æˆªæ–­è¿˜æ˜¯æ»‘åŠ¨çª—å£
- å¤šè½®é—®ç­”æ‹†æˆå•è½®é—®ç­”äº†,æœ€ç»ˆæ¦‚ç‡æ€ä¹ˆè®¡ç®—
- å¤šå¡è®­ç»ƒ



## License

MIT