import os
from time import time
import pandas as pd
import numpy as np
import logging
from sklearn.model_selection import train_test_split

import torch
import torch.distributed as dist
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    TrainerCallback
)
import wandb

from configs.logging_config import make_log_dir, init_logger
from configs import CONFIG, print_config
from dataset import HumanPreferenceDataset

os.environ['TOKENIZERS_PARALLELISM'] = 'false'

class DetailedLoggingCallback(TrainerCallback):
    
    def __init__(self, log_every_n_steps=50):
        self.log_every_n_steps = log_every_n_steps
        self.start_time = None
        self.is_main_process = True
        
    def on_train_begin(self, args, state, control, **kwargs):
        import time
        self.start_time = time.time()
        self.is_main_process = state.is_world_process_zero
        
    def on_epoch_begin(self, args, state, control, **kwargs):
        pass
        # if self.is_main_process:
        #     logging.info(f">>> Epoch {state.epoch}/{args.num_train_epochs} å¼€å§‹")
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        pass
    
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if self.is_main_process and metrics:
            logging.info("=" * 80)
            logging.info(f"ğŸ“Š è¯„ä¼°ç»“æœ (Epoch {int(state.epoch)}):")
            for key, value in metrics.items():
                if isinstance(value, (int, float)):
                    logging.info(f"  {key}: {value:.4f}")
            logging.info("=" * 80)
    
    def on_save(self, args, state, control, **kwargs):
        if self.is_main_process:
            logging.info(f"ğŸ’¾ ä¿å­˜checkpointåˆ°: {args.output_dir}/checkpoint-{state.global_step}")


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    accuracy = (predictions == labels).mean()
    
    unique_labels = np.unique(labels)
    class_accuracies = {}
    for label in unique_labels:
        mask = labels == label
        if mask.sum() > 0:
            class_acc = (predictions[mask] == labels[mask]).mean()
            class_accuracies[f'accuracy_class_{label}'] = class_acc
    
    return {
        "accuracy": accuracy,
        **class_accuracies
    }


def print_model_info(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    logging.info("=" * 80)
    logging.info("ğŸ”§ æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    logging.info(f"  æ€»å‚æ•°: {total_params:,}")
    logging.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
    logging.info(f"  å†»ç»“å‚æ•°: {frozen_params:,} ({frozen_params/total_params*100:.2f}%)")
    logging.info("=" * 80 + "\n")


def setup_ddp():
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
    else:
        rank = 0
        world_size = 1
        local_rank = 0
    
    return rank, world_size, local_rank


def is_main_process():
    rank = int(os.environ.get('RANK', 0))
    return rank == 0


def main():
    # ============ DDP è®¾ç½® ============
    use_ddp = CONFIG.get('use_ddp', False)
    rank, world_size, local_rank = setup_ddp()
    is_main = is_main_process()
    
    # åªåœ¨ä¸»è¿›ç¨‹åˆå§‹åŒ–loggerå’Œæ‰“å°é…ç½®
    if is_main:
        init_logger(make_log_dir())
        print_config()
        wandb.init(
            project='human-preference-prediction',
            config=CONFIG,
            name=f"train-deberta-lr{CONFIG['learning_rate']:.1e}-bs{CONFIG['batch_size']}-ep{CONFIG['num_epochs']}"
        )
    
    # è®¾ç½®è®¾å¤‡
    if use_ddp:
        device = torch.device(f'cuda:{local_rank}')
        torch.cuda.set_device(device)
        
        logging.info(f'World Size: {world_size}')
        for i in range(torch.cuda.device_count()):
            logging.info(f'GPU {i}: {torch.cuda.get_device_name(i)} '
                        f'({torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB)')
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logging.info(f'ä½¿ç”¨è®¾å¤‡: {device}')
        if torch.cuda.is_available():
            logging.info(f'GPUå‹å·: {torch.cuda.get_device_name(0)}')
            logging.info(f'GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')
    
    # ============ æ¨¡å‹åˆå§‹åŒ– ============
    if is_main:
        logging.info(f'åˆå§‹åŒ–æ¨¡å‹: {CONFIG["model_name"]}')
    
    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])
   
    model = AutoModelForSequenceClassification.from_pretrained(
        CONFIG['model_name'],
        num_labels=3,
        hidden_dropout_prob=0.1,
        attention_probs_dropout_prob=0.1,
    )   
    
    for param in model.deberta.embeddings.parameters():
        param.requires_grad = False
    num_layers_to_freeze = 10  
    for i, layer in enumerate(model.deberta.encoder.layer):
        if i < num_layers_to_freeze:
            for param in layer.parameters():
                param.requires_grad = False
    for param in model.classifier.parameters():
        param.requires_grad = True
    
    if is_main:
        print_model_info(model)
    
    # ============ æ•°æ®åŠ è½½ ============
    if is_main:
        logging.info('åŠ è½½è®­ç»ƒæ•°æ®...')
    
    train_df = pd.read_csv(CONFIG['train_dataset_path']) if not CONFIG['develop'] else pd.read_csv('data/train_short.csv')
    
    if is_main:
        logging.info('åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†...')
    
    train_data, val_data = train_test_split(
        train_df,
        test_size=CONFIG['val_rate'],
        random_state=CONFIG['seed'],
        stratify=train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)
    )
    
    # ============ åˆ›å»ºæ•°æ®é›† ============
    train_dataset = HumanPreferenceDataset(
        data=train_df,
        tokenizer=tokenizer,
        max_length=CONFIG['max_length'],
        prompt_ratio=CONFIG['prompt_ratio'],
        cache_dir="./data",
        force_reprocess=False,
        usage="train"
    )
    val_dataset = HumanPreferenceDataset(
        data=val_data,
        tokenizer=tokenizer,
        max_length=CONFIG['max_length'],
        prompt_ratio=CONFIG['prompt_ratio'],
        cache_dir="./data",
        force_reprocess=False,
        usage="val"
    )
    
    # ============ è®­ç»ƒé…ç½® ============
    # è®¡ç®—æœ‰æ•ˆçš„batch sizeå’Œsteps
    effective_batch_size = CONFIG['batch_size']
    gradient_accumulation_steps = CONFIG.get('gradient_accumulation_steps', 1)
    
    if use_ddp:
        # DDPä¸‹çš„å®é™…batch size = per_device_batch_size * num_gpus * gradient_accumulation_steps
        total_batch_size = effective_batch_size * world_size * gradient_accumulation_steps
    else:
        total_batch_size = effective_batch_size * gradient_accumulation_steps
    
    steps_per_epoch = len(train_dataset) // total_batch_size
    total_steps = steps_per_epoch * CONFIG['num_epochs']
    warmup_steps = int(total_steps * CONFIG['warmup_ratio'])
    
    if is_main:
        logging.info(f'è®­ç»ƒæ­¥æ•°é…ç½®:')
        logging.info(f'  Per device batch size: {effective_batch_size}')
        if use_ddp:
            logging.info(f'  Number of GPUs: {world_size}')
        logging.info(f'  Gradient accumulation steps: {gradient_accumulation_steps}')
        logging.info(f'  Total batch size: {total_batch_size}')
        logging.info(f'  æ¯epochæ­¥æ•°: {steps_per_epoch}')
        logging.info(f'  æ€»è®­ç»ƒæ­¥æ•°: {total_steps}')
        logging.info(f'  é¢„çƒ­æ­¥æ•°: {warmup_steps}')
    
    # ============ TrainingArguments è¯¦ç»†é…ç½® ============
    training_args = TrainingArguments(
        output_dir=CONFIG['checkpoint_dir'],
        
        # === è®­ç»ƒé…ç½® ===
        num_train_epochs=CONFIG['num_epochs'],
        per_device_train_batch_size=CONFIG['batch_size'],
        per_device_eval_batch_size=CONFIG['batch_size'],
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=CONFIG['learning_rate'],
        weight_decay=CONFIG['weight_decay'],
        warmup_steps=warmup_steps,
        lr_scheduler_type="cosine",
        max_grad_norm=4.0,
        
        # === DDPé…ç½® ===
        ddp_find_unused_parameters=False,
        ddp_backend='nccl' if use_ddp and torch.cuda.is_available() else None,
        
        # === è¯„ä¼°ç­–ç•¥ ===
        eval_strategy="epoch",
        
        # === Checkpointä¿å­˜ç­–ç•¥ ===
        save_strategy="epoch",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        
        # === Loggingé…ç½® ===
        logging_dir=f"{CONFIG['checkpoint_dir']}/logs",
        logging_strategy="steps",
        logging_steps=20,
        logging_first_step=True,
        
        # === æ··åˆç²¾åº¦è®­ç»ƒ ===
        fp16=CONFIG.get('use_amp', False) and torch.cuda.is_available(),
        # bf16=True,
        
        # === å…¶ä»–è®¾ç½® ===
        dataloader_num_workers=CONFIG.get('num_workers', 4),
        dataloader_pin_memory=True,
        remove_unused_columns=False,
        seed=CONFIG['seed'],
        
        # === æŠ¥å‘Šå·¥å…· ===
        report_to=["wandb"],
        
        # === åˆ†å¸ƒå¼ç›¸å…³ ===
        local_rank=local_rank if use_ddp else -1,  # é‡è¦ï¼šå‘Šè¯‰Trainerå½“å‰è¿›ç¨‹çš„local_rank
    )
    
    if is_main:
        if CONFIG.get('use_amp', False):
            logging.info('âœ“ å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)')
        
        if training_args.report_to != "none":
            logging.info(f'âœ“ å¯ç”¨å®éªŒè¿½è¸ª: {training_args.report_to}')

    # ============ Data Collator ============
    def custom_data_collator(features):
        batch = {
            'input_ids': torch.stack([f['input_ids'] for f in features]),
            'attention_mask': torch.stack([f['attention_mask'] for f in features]),
            'labels': torch.stack([f['labels'] for f in features])
        }
        return batch
    
    # ============ åˆ›å»ºTrainer ============
    callbacks = [DetailedLoggingCallback(log_every_n_steps=50)]
    
    if CONFIG.get('early_stopping', False):
        callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))
        if is_main:
            logging.info('âœ“ å¯ç”¨æ—©åœæœºåˆ¶ (patience=3)')
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=custom_data_collator,
        compute_metrics=compute_metrics,
        callbacks=callbacks,
    )
    

    train_result = trainer.train()
    
    # ============ ä¿å­˜æœ€ç»ˆæ¨¡å‹ (åªåœ¨ä¸»è¿›ç¨‹) ============
    if is_main:
        logging.info("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        final_model_dir = f"{CONFIG['checkpoint_dir']}/best_model"
        trainer.save_model(final_model_dir)
        tokenizer.save_pretrained(final_model_dir)
        logging.info(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_dir}")
        
        # ============ ä¿å­˜è®­ç»ƒæŒ‡æ ‡ ============
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        
        # ============ è®­ç»ƒæ€»ç»“ ============
        logging.info("=" * 80)
        logging.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        logging.info("=" * 80)
        logging.info(f"è®­ç»ƒæŸå¤±: {metrics.get('train_loss', 'N/A'):.4f}")
        logging.info(f"æœ€ä½³æ¨¡å‹: {final_model_dir}")
        logging.info("=" * 80 + "\n")


if __name__ == '__main__':
    main()